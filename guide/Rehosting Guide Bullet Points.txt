General Process Notes:


TODO: Add a percentage assignment to each task. This gives the Project Manager a better idea of how many people he/she should assign to each task especially if they are new or have never done a migration before.


1. Pre Migration Tasks
	a. Questionnaire
	b. Source Transfer (FTP from Mainframe to Linux)
		i. JCL
		ii. PROC
		iii. COBOL
		iv. COPYBOOK
		v. CSD
	c. Installation (for analysis, see 1a.)
	d. Analysis
		i. OFMiner
		ii. TBAdmin (To create SCOPE spreadsheet)

1a. Installation

2. Migration
	a. convcpy (copybooks)
	b. datasets
		i. NON-VSAM
		ii. VSAM
	c. source code (FTP from Mainframe to Linux)
		i. JCL
		ii. PROC
		iii. COBOL
		iv. COPYBOOK
		v. CSD
	d. dsmigin
		i. ds_wrap.sh
			- PREREQUISITES: This script requires an input file with the names of the datasets that you want to FTP from the mainframe to OpenFrame.
			a. dsn2ftp.sh
				formats dataset names for FTP format
			b. MyFtpJar.jar
				- Checks the passed file list to see if the list is greater than 0
				- If the passed filed list is greater than 0, makes a connection to the mainframe
				- Checks the mainframe for the list of datasets
				- If the dataset is found, passes the name to tmaxmvsfilelist.txt
					- If the dataset is migrated, passes the name to tmaxmvsfilelist_migrated.txt
			c. po-list2lftpcmd.awk
				- This script formats and writes the commands into the getfiles.sh script in FTP syntax.
			d. getfiles.sh
				- This script can be run to physically retreive the datasets created in the input file.
		ii. data_dsmigin.sh
			- After FTP'ing the datasets from mainframe to OpenFrame, the datasets are downloaded to the data directory (specified in the configuration file that comes with the data_dsmigin.sh script called data_dsmigin.conf). You will need to pass the base name of the dataset. The base name of the dataset, is the dataset name excluding any GDG version number and/or information created from the po-list2lftpcmd.awk script. 
				Example: FULL.DATASET.NAME.PO.FB_80
				- The base name of the above dataset is "FULL.DATASET.NAME"
			- Additionally, you will have to provide the schema file that you will be using to dsmigin the dataset. If the type of dsmigin you are doing does not require a schema, you can ignore this option. 
				#TODO: EXAMPLE
3. OpenFrame Configuration
	Some of this configuration will have to be done before migration in order to get the analysis from OFMiner. We need to know what configurations are priority and which configurations are not priority so that we can work on them first and figure out the rest as we go.
	- cpm.conf
	- dbutil.conf
	- ds.conf
	- dstool.conf
	- ezaci.conf
	- ezplus.conf
	- ftp.conf
	- hidb.conf
	- idcams.conf
	- ikjeft01.conf
	- ims.conf
	- isrsupc.conf
	- keyseq.conf
	- ofosc.seq
	- ofstudio.conf
	- ofsys.seq
	- osc.OSCOIVP1.conf
	- osc.OSCOIVP1TL.conf
	- osc.conf
	- osc.lu.conf
	- osc.region.list
	- osc.IMSA.conf
	- osi.conf
	- osi.ofsys.seq
	- osi.ofsys.seq_for_OSI_ONLY
	- osi.ofsys.seq_orig
	- print.conf
	- rc.conf
	- saf.conf
	- smf.conf
	- sms.conf
	- sort.conf
	- ssm.IMSADB2T.conf
	- tacf.conf
	- textrun.conf
	- tjclrun.conf
	- tjes.conf
	- tjesmgr.conf
	- tso.conf
	- unit.conf
	- volume.conf
	- vtam.conf

3a. Compilation
	BATCH
		COBOL
		ASM
		PL/I
	ONLINE
		OSC
			COBOL
			ASSEMBLER
			BMS
		OSI
			COBOL
			ASSEMBLER
			MFS


4. Security
	Questions we need to ask:
		- What type of security is the customer using?
			- ACF2
			- RACF
			- ...
		- Do we support the conversion of the mainframe security to OpenFrame TACF?
		- What are the mainframe commands to dump the current security information so that we can migrate it into TACF
		- What are the equivalent commands to create the same security restrictions in OpenFrame TACF?
	Customer Security --> TACF

5. Running Batch JOBs
	Prerequisities
		- Configuration
		- Migration
		- Security (??)
	Need to determine the JOB Streams (Order in which the JOBs are run on the mainframe)

	Need to understand how tjesmgr and textrun works

	

6. JOB Stream and Schedule
	Once all of the jobs have been successfully tested individually, we need to begin to test them in stream. The JOBs will be run in stream end to end and the output datasets are to be compared for approval.



7. Operations & Maintenance
	- Spool
		- Backup
			- auto_backup_spool.sh
		- Restore
	- 


XX. Approval

			